Models:
  Embeddings:
    model_name: "all-MiniLM-L6-v2.gguf2.f16.gguf"

  LLM:
    llm_path: "models/Meta-Llama-3.1-8B-Instruct-Q3_K_M.gguf"
    context_length: 2048
    n_threads: 4

  Rag:
    k: 5 #no of answers to retrieve
    chunk_size: 256
    chunk_overlap: 32
  
  GROQ:
    model_name: "mixtral-8x7b-32768"
    temperature: 0.8
    max_tokens: None
    timeout: None
    max_tries: 2
